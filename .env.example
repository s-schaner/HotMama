# Deployment profile: cpu, gpu, or rocm
PROFILE=cpu

# Redis connection string shared by services
REDIS_URL=redis://queue:6379/0
REDIS_QUEUE_NAME=hotmama:jobs
REDIS_STATUS_PREFIX=hotmama:job

# Location for generated artifacts (relative to project root in compose)
ARTIFACT_DIR=/app/sessions

# Enable JSON logging by setting to 1
LOG_JSON=0

# GUI Configuration
GUI_API_BASE_URL=http://api:8000/v1
GUI_PORT=7860
GUI_HOST=0.0.0.0

# LM Studio Configuration (local LLM inference)
# Set to your LM Studio endpoint, default is local at 127.0.0.1:1234
GUI_LMSTUDIO_BASE_URL=http://127.0.0.1:1234
GUI_LMSTUDIO_API_KEY=lm-studio
GUI_LM_PARSER_MODEL=qwen2.5-3b-instruct
GUI_LM_VISION_MODEL=qwen/qwen2.5-vl-7b
GUI_LM_ENRICHMENT_MODEL=qwen2.5-vl-7b
GUI_LM_TEMPERATURE=0.0
GUI_LM_MAX_TOKENS=512

# Hugging Face Configuration (cloud LLM inference)
# Leave blank to use LM Studio only
# GUI_HUGGINGFACE_API_URL=https://api-inference.huggingface.co/models/YOUR_MODEL
# GUI_HUGGINGFACE_API_KEY=your_hf_api_key_here
# GUI_HUGGINGFACE_MODEL=your_model_name

# LLM Provider Selection: lmstudio or huggingface
GUI_LLM_PROVIDER=lmstudio
